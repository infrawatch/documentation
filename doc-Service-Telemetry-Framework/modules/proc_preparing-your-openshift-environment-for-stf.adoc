// Module included in the following assemblies:
//
// <List assemblies here, each on a new line>

// This module can be included from assemblies using the following include statement:
// include::<path>/proc_preparing-your-openshift-environment-for-stf.adoc[leveloffset=+1]

// The file name and the ID are based on the module title. For example:
// * file name: proc_doing-procedure-a.adoc
// * ID: [id='proc_doing-procedure-a_{context}']
// * Title: = Doing procedure A
//
// The ID is used as an anchor for linking to the module. Avoid changing
// it after the module has been published to ensure existing links are not
// broken.
//
// The `context` attribute enables module reuse. Every module's ID includes
// {context}, which ensures that the module has a unique ID even if it is
// reused multiple times in a guide.
//
// Start the title with a verb, such as Creating or Create. See also
// _Wording of headings_ in _The IBM Style Guide_.
[id='preparing-your-openshift-environment-for-stf_{context}']
= Preparing your {OpenShiftShort} environment for {ProjectShort}

As you prepare your {OpenShiftShort} environment for {ProjectShort}, you must plan for persistent storage, adequate resources, and event storage:

* Ensure that persistent storage is available in your {OpenShift} cluster for storing data in a production system. For more information, see <<persistent-volumes>>.
* Ensure that enough resources are available to run the Operators and the application containers. For more information, see <<resource-allocation>>.
* To install ElasticSearch, you must use a community catalog source. If you do not want to use a community catalog or if you do not want to store events, see <<Deploying STF to the OKD environment>>.
* `vm_max_count` has been set for nodes that ElasticSearch will be scheduled to using the Node Tuning Operator. For more information, see <<node-tuning-operator>>.



[[persistent-volumes]]
== Persistent volumes

//This is a prerequisite.
//This is something you have to do as part of your planning for your OpenShift install.

{ProjectShort} uses persistent storage in {OpenShiftShort} to instantiate the volumes dynamically for Prometheus and ElasticSearch to store metrics and events. For more information about configuring persistent storage for {OpenShiftShort}, see https://docs.openshift.com/container-platform/4.3/storage/understanding-persistent-storage.html

[[ephemeral-storage]]
=== Using ephemeral storage

It's possible to use ephemeral storage with {ProjectShort} with a caveat that it can result in data loss should a pod be restarted, updated, or rescheduled onto another node. Due the volatility of ephemeral storage it is only recommended to be used for development or testing, and not production environments.

To enable ephemeral storage for {ProjectShort} set `storageEphemeralEnabled: true` in your `ServiceTelemetry` manifest. For more information about enabling ephemeral storage for {ProjectShort}, see <<configuring-ephemeral-storage>>

[[resource-allocation]]
== Resource allocation

//This is a prerequisite.
//This is something you have to do as part of your planning for your OpenShift install.

To enable the scheduling of pods within the OpenShift infrastructure, you need resources for the components that are running and for the number of running resources. If you do not allocate enough resources, pods remain in a `Pending` state because they cannot be scheduled.

The amount of resources you require to run {ProjectShort} depends on your environment and the number of nodes and clouds you are monitoring. For recommendations around sizing for metrics collection see https://access.redhat.com/articles/4907241. For information about sizing ElasticSearch, see https://www.elastic.co/guide/en/cloud-on-k8s/current/k8s-managing-compute-resources.html

[[disabling-event-storage]]
== Disabling event storage

If do not plan to store events in ElasticSearch, do not enable the OperatorHub.io Community Catalog Source and do not subscribe to the Elastic Cloud on Kubernetes Operator. You must set events_enabled to false in your ST object. For more information, see Installing the core components of STF.


[[node-tuning-operator]]
== Node tuning operator

//vm_max_count is set by default.  If you're using Opensift 4.3 don't worry. By default, it will work. If you created other types of OpenShift nodes, those nodes get listed in an inventory in OS. This has metadata, for example, what type of node is this. When you scehdule an ES process on the node, it has metata, the lable says I'm an ES. When you put that lable, OS says you're scheduling a process with this label, and when that process is scheduled on to a node, I need to adjust something on a machine. this is automatic.  When the label is present, OpenShift takes care of it.

{ProjectShort} uses ElasticSearch to store events, which requires a larger than normal `vm.max_map_count`. For more information about virtual memory usage by ElasticSearch, see https://www.elastic.co/guide/en/elasticsearch/reference/current/vm-max-map-count.html

You cannot apply node tuning, such as virtual memory map count values, manually using the `sysctl` command because {OpenShift} manages nodes directly. To configure values and apply them to the infrastructure, you must use the node tuning operator. For more information about the node tuning operator, see https://docs.openshift.com/container-platform/4.3/scalability_and_performance/using-node-tuning-operator.html[Using the Node Tuning Operator].

In an {OpenShiftShort} deployment, the default node tuning operator specification provides the required profiles for ElasticSearch workloads or pods scheduled on nodes. To view the default cluster node tuning, specification run the following command:

----
oc get Tuned/default -o yaml -n openshift-cluster-node-tuning-operator
----

The output of the default specification is documented at https://docs.openshift.com/container-platform/4.3/scalability_and_performance/using-node-tuning-operator.html#custom-tuning-default-profiles-set_node-tuning-operator[Default profiles set on a cluster]. Assignment of profiles is managed in the `recommend` section where profiles are applied to a node when certain conditions are met. When scheduling ElasticSearch to a node in {ProjectShort}, one of the following profiles is applied:

* `openshift-control-plane-es`
* `openshift-node-es`

When scheduling an ElasticSearch pod, there must be a label matching `tuned.openshift.io/elasticsearch` present. If the label is present, one of the two profiles is assigned to the pod. No action is required by the administrator if you use the recommended Operator for ElasticSearch. If you use a custom-deployed ElasticSearch with {ProjectShort}, ensure that you add the `tuned.openshift.io/elasticsearch` label to all scheduled pods.

For more information about how the profiles are applied to nodes, see https://docs.openshift.com/container-platform/4.3/scalability_and_performance/using-node-tuning-operator.html#custom-tuning-specification_node-tuning-operator
