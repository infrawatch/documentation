// Module included in the following assemblies:
//
// <List assemblies here, each on a new line>

// This module can be included from assemblies using the following include statement:
// include::<path>/proc_preparing-your-openshift-environment-for-stf.adoc[leveloffset=+1]

// The file name and the ID are based on the module title. For example:
// * file name: proc_doing-procedure-a.adoc
// * ID: [id='proc_doing-procedure-a_{context}']
// * Title: = Doing procedure A
//
// The ID is used as an anchor for linking to the module. Avoid changing
// it after the module has been published to ensure existing links are not
// broken.
//
// The `context` attribute enables module reuse. Every module's ID includes
// {context}, which ensures that the module has a unique ID even if it is
// reused multiple times in a guide.
//
// Start the title with a verb, such as Creating or Create. See also
// _Wording of headings_ in _The IBM Style Guide_.
[id='preparing-your-openshift-environment-for-stf_{context}']
= Preparing your {OpenShiftShort} environment for {ProjectShort}

To prepare our OKD environment for STF we need to consider the following:

* Persistent volumes are available for data storage
* `vm_max_count` has been set for nodes that ElasticSearch will be scheduled to using the Node Tuning Operator
* Enough resources are available to run the Operators and the application containers

== Persistent volumes

STF uses persistent storage in OKD to instantiate the volumes dynamically for Prometheus and ElasticSearch to store metrics and events. More information about configuring persistent storage for OKD can be found at https://docs.openshift.com/container-platform/4.3/storage/understanding-persistent-storage.html

== Node Tuning Operator

ElasticSearch is used for storing events in STF and requires a larger than normal `vm.max_map_count`. See https://www.elastic.co/guide/en/elasticsearch/reference/current/vm-max-map-count.html for more information about virtual memory usage by ElasticSearch.

Node tuning such as virtual memory map count values can not be applied manually via `sysctl` since nodes are managed by OpenShift directly. To configure values and apply them to the infrastructure, the node tuning operator is used. For more information about the node tuning operator see https://docs.openshift.com/container-platform/4.3/scalability_and_performance/using-node-tuning-operator.html[Using the Node Tuning Operator].

In an {OpenShiftShort} deployment, the default node tuning operator specification provides the required profiles for ElasticSearch workloads (pods) scheduled on nodes. To view the default cluster node tuning specification run the following command:

[source,bash]
----
oc get Tuned/default -o yaml -n openshift-cluster-node-tuning-operator
----

The output of the default specification is documented at https://docs.openshift.com/container-platform/4.3/scalability_and_performance/using-node-tuning-operator.html#custom-tuning-default-profiles-set_node-tuning-operator[Default profiles set on a cluster]. Assignment of profiles is managed in the `recommend` section where profiles are applied to a node when certain conditions are met. When scheduling ElasticSearch to a node in {ProjectShort}, one of two profiles are applied:

* `openshift-control-plane-es`
* `openshift-node-es`

When scheduling an ElasticSearch pod, there must be a label matching tuned.openshift.io/elasticsearch present. If the label is present, one of the two profiles is assigned to the pod. No action is required by the administrator if you use the recommended Operator for ElasticSearch. If you use a custom-deployed ElasticSearch with {ProjectShort}, ensure that you add the tuned.openshift.io/elasticsearch label to any scheduled pods.

More information about how the profiles are applied to nodes is available at https://docs.openshift.com/container-platform/4.3/scalability_and_performance/using-node-tuning-operator.html#custom-tuning-specification_node-tuning-operator

== Resource allocation

Resources for the components being run (and the number of them running) are required to allow the pods to be scheduled within the OpenShift infrastructure. If enough resources are not allocated, pods will remain in a `Pending` state due to not be schedulable.

The amount of resources required for running STF is variable depending on your environment and the number of nodes and clouds you're monitoring. For recommendations around sizing for metrics collection see https://access.redhat.com/articles/4907241. For information about sizing ElasticSearch, see https://www.elastic.co/guide/en/cloud-on-k8s/current/k8s-managing-compute-resources.html
