// Module included in the following assemblies:
//
// <List assemblies here, each on a new line>

// This module can be included from assemblies using the following include statement:
// include::<path>/proc_configuring-red-hat-openstack-platform-overcloud-for-stf.adoc[leveloffset=+1]

// The file name and the ID are based on the module title. For example:
// * file name: proc_doing-procedure-a.adoc
// * ID: [id='proc_doing-procedure-a_{context}']
// * Title: = Doing procedure A
//
// The ID is used as an anchor for linking to the module. Avoid changing
// it after the module has been published to ensure existing links are not
// broken.
//
// The `context` attribute enables module reuse. Every module's ID includes
// {context}, which ensures that the module has a unique ID even if it is
// reused multiple times in a guide.
//
// Start the title with a verb, such as Creating or Create. See also
// _Wording of headings_ in _The IBM Style Guide_.
[id="configuring-red-hat-openstack-platform-overcloud-for-stf_{context}"]
= Configuring {OpenStack} overcloud for {Project}

[role="_abstract"]
To configure the {OpenStack} overcloud, you must configure the data collection applications and the data transport to {ProjectShort}, and deploy the overcloud.

To configure the {OpenStack} overcloud, complete the following tasks:

. xref:retrieving-the-qdr-route-address[]
. xref:configuring-the-stf-connection-for-the-overcloud[]
. xref:validating-clientside-installation_assembly-completing-the-stf-configuration[]

ifdef::include_when_16_1[]
.Additional resources

* To collect data through {MessageBus}, see https://access.redhat.com/documentation/en-us/red_hat_openstack_platform/16.1/html-single/monitoring_tools_configuration_guide/index#amqp1[The amqp1 plug-in] in the _Monitoring Tools Configuration_ guide.

endif::include_when_16_1[]

[[retrieving-the-qdr-route-address]]
== Retrieving the {MessageBus} route address

When you configure the {OpenStack} overcloud for {ProjectShort}, you must provide the {MessageBus} route address in the {ProjectShort} connection file.

.Procedure

. Log in to your {OpenShift} ({OpenShiftShort}) environment.

. In the `service-telemetry` project, retrieve the {MessageBus} route address:
+
[source,bash,options="nowrap",subs="verbatim"]
----
$ oc get routes -ogo-template='{{ range .items }}{{printf "%s\n" .spec.host }}{{ end }}' | grep "\-5671"
default-interconnect-5671-service-telemetry.apps.infra.watch
----
+
[NOTE]
If your {ProjectShort} installation differs from the documentation, ensure that you retrieve the correct {MessageBus} route address.

[[configuring-the-stf-connection-for-the-overcloud]]
== Configuring the {ProjectShort} connection for the overcloud

To configure the {ProjectShort} connection, you must create a file that contains the connection configuration of the {MessageBus} for the overcloud to the {ProjectShort} deployment. Enable the collection of events and storage of the events in {ProjectShort} and deploy the overcloud.

.Procedure

. Log in to the {OpenStack} undercloud as the `stack` user.

. Create a configuration file called `stf-connectors.yaml` in the `/home/stack` directory.
+
[IMPORTANT]
====
The Service Telemetry Operator simplifies the deployment of all data ingestion and data storage components for single cloud deployments. To share the data storage domain with multiple clouds, see xref:configuring-multiple-clouds_assembly-advanced-features[].

Additionally, setting `EventPipelinePublishers` and `PipelinePublishers` to empty lists results in no metric or event data passing to {OpenStack} legacy telemetry components, such as Gnocchi or Panko. If you need to send data to additional pipelines, the Ceilometer polling interval of 5 seconds as specified in `ExtraConfig` might overwhelm the legacy components. If you configure a longer polling interval, you must also modify {ProjectShort} to avoid stale metrics, resulting in what appears to be missing data in Prometheus.

If an adjustment needs to be made to the polling interval, then modify the ServiceTelemetry object `backends.metrics.prometheus.scrapeInterval` parameter from the default value of `10s` to double the polling interval of the data collectors. For example, if `CollectdAmqpInterval` and `ceilometer::agent::polling::polling_interval` are adjusted to `30` then set the `backends.metrics.prometheus.scrapeInterval` to a value of `60s`.
====

. In the `stf-connectors.yaml` file, configure the `MetricsQdrConnectors` address to connect the {MessageBus} on the overcloud to the {ProjectShort} deployment.
* Add the `CeilometerQdrPublishMetrics: true` parameter to enable collection and transport of Ceilometer metrics to {ProjectShort}.
* Add the `CeilometerQdrPublishEvents: true` parameter to enable collection and transport of Ceilometer events to {ProjectShort}.
* Add the `EventPiplinePublishers: []` and `PipelinePublishers: []` to avoid writing data to Gnocchi and Panko.
* Add the `ManagePolling: true` and `ManagePipeline: true` parameters to allow full control of Ceilometer polling and pipeline configuration.
* Add the `ExtraConfig` parameter `ceilometer::agent::polling::polling_interval` to set the polling interval of Ceilometer to be compatible with the default {ProjectShort} scrape interval.
* Replace the `host` parameter with the value of `HOST/PORT` that you retrieved in xref:retrieving-the-qdr-route-address[]:
+
ifdef::include_when_13[]
[source,yaml]
----
parameter_defaults:
    EventPipelinePublishers: []
    PipelinePublishers: []
    CeilometerEnablePanko: false
    CeilometerQdrPublishEvents: true
    CeilometerQdrPublishMetrics: true
    ManagePipeline: true
    ManagePolling: true
    CollectdAmqpInstances:
        notify:
            format: JSON
            notify: true
            presettle: false
        telemetry:
            format: JSON
            presettle: false
    CollectdAmqpInterval: 5
    CollectdConnectionType: amqp1
    CollectdDefaultPlugins:
    - cpu
    - df
    - disk
    - hugepages
    - interface
    - load
    - memory
    - processes
    - unixsock
    - uptime
    - connectivity
    - intel_rdt
    - ipmi
    - procevent
    CollectdDefaultPollingInterval: 5
    MetricsQdrAddresses:
    -   distribution: multicast
        prefix: collectd
    -   distribution: multicast
        prefix: anycast/ceilometer
    MetricsQdrConnectors:
    -   host: default-interconnect-5671-service-telemetry.apps.infra.watch
        port: 443
        role: edge
        sslProfile: sslProfile
        verifyHostname: false
    MetricsQdrSSLProfiles:
ifdef::include_when_13[]
    -   name: sslProfile
        caCertFileContent: |
          ----BEGIN CERTIFICATE----
          <snip>
          ----END CERTIFICATE----
endif::include_when_13[]
    ExtraConfig:
        collectd::plugin::cpu::reportbycpu: true
        collectd::plugin::cpu::reportbystate: true
        collectd::plugin::cpu::reportnumcpu: false
        collectd::plugin::cpu::valuespercentage: true
        collectd::plugin::df::ignoreselected: true
        collectd::plugin::df::reportbydevice: true
        collectd::plugin::df::fstypes: ['xfs']
        collectd::plugin::load::reportrelative: true
        collectd::plugin::virt::connection: "qemu:///system"
        collectd::plugin::virt::extra_stats: "cpu_util disk disk_err pcpu job_stats_background perf vcpupin"
        collectd::plugin::virt::hostname_format: "hostname"
        ceilometer::agent::polling::polling_interval: 5
----
endif::include_when_13[]
ifdef::include_when_16[]
[source,yaml]
----
parameter_defaults:
    EventPipelinePublishers: []
    PipelinePublishers: []
    CeilometerQdrPublishEvents: true
    CeilometerQdrPublishMetrics: true
    MetricsQdrConnectors:
    - host: default-interconnect-5671-service-telemetry.apps.infra.watch
      port: 443
      role: edge
      sslProfile: sslProfile
      verifyHostname: false
    ExtraConfig:
      ceilometer::agent::polling::polling_interval: 5
----
endif::include_when_16[]

. Add the following files to your {OpenStack} {OpenStackInstaller} deployment to setup collectd and {MessageBus}:
+
* the `stf-connectors.yaml` environment file
ifdef::include_when_16[* the `enable-stf.yaml` file that ensures that the environment is being used during the overcloud deployment]
ifdef::include_when_13[* the `collectd-write-qdr.yaml` file that ensures that collectd telemetry is sent to {ProjectShort}]
* the `ceilometer-write-qdr.yaml` file that ensures that Ceilometer telemetry is sent to {ProjectShort}
+
ifdef::include_when_13[]
[source,bash,options="nowrap",subs="+quotes"]
----
openstack overcloud deploy <other arguments>
--templates /usr/share/openstack-tripleo-heat-templates \
  --environment-file <...other-environment-files...> \
  --environment-file /usr/share/openstack-tripleo-heat-templates/environments/metrics/ceilometer-write-qdr.yaml \
  --environment-file /usr/share/openstack-tripleo-heat-templates/environments/metrics/collectd-write-qdr.yaml \
  --environment-file /usr/share/openstack-tripleo-heat-templates/environments/metrics/qdr-edge-only.yaml \
  --environment-file /home/stack/stf-connectors.yaml
----
endif::include_when_13[]
ifdef::include_when_16[]
[source,bash,options="nowrap",subs="+quotes"]
----
openstack overcloud deploy <other arguments>
  --templates /usr/share/openstack-tripleo-heat-templates \
  --environment-file <...other-environment-files...> \
  --environment-file /usr/share/openstack-tripleo-heat-templates/environments/metrics/ceilometer-write-qdr.yaml \
  --environment-file /usr/share/openstack-tripleo-heat-templates/environments/enable-stf.yaml \
  --environment-file /home/stack/stf-connectors.yaml
----
endif::include_when_16[]

. Deploy the {OpenStack} overcloud.
