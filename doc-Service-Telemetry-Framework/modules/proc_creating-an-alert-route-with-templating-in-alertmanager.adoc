
[id="creating-an-alert-route-with-templating-in-alertmanager_{context}"]
= Creating an alert route with templating in Alertmanager

// The introduction to the files proc_creating-an-alert-route-with-templating-in-alertmanager and proc_creating-an-alert-route-in-alertmanager are identical. If you have changes to make, please make the same changes to both introductions.

[role="_abstract"]
Use Alertmanager to deliver alerts to an external system, such as email, IRC, or other notification channel. The Prometheus Operator manages the Alertmanager configuration as a {OpenShift} secret. By default, {Project} ({ProjectShort}) deploys a basic configuration that results in no receivers:

[source,yaml]
----
alertmanager.yaml: |-
  global:
    resolve_timeout: 5m
  route:
    group_by: ['job']
    group_wait: 30s
    group_interval: 5m
    repeat_interval: 12h
    receiver: 'null'
  receivers:
  - name: 'null'
----

If the `alertmanagerConfigManifest` parameter contains a custom template, for example, to construct the title and text of the sent alert, you must deploy the contents of the `alertmanagerConfigManifest` by using a base64-encoded configuration.

.Procedure

// The following steps are duplicated in proc_creating-an-alert-route-in-alertmanager. If you have changes to make, please make the same changes to both files.

. Log in to {OpenShift}.
. Change to the `service-telemetry` namespace:
+
[source,bash]
----
$ oc project service-telemetry
----

. Create the necesarry alermanager config in a file called alertmanager.yaml, for example:
+
[source,yaml]
----
$ cat alertmanager.yaml
global:
  resolve_timeout: 10m
  slack_api_url: <slack_api_url>
receivers:
  - name: slack
    slack_configs:
    - channel: #stf-alerts
      title: |-
        ...
      text: >-
        ...
route:
  group_by: ['job']
  group_wait: 30s
  group_interval: 5m
  repeat_interval: 12h
  receiver: 'slack'
----
. Generate the config manifest and add it to the `ServiceTelemetry` object for your {ProjectShort} deployment:
+
[source,bash,options="nowrap"]
----
$ CONFIG_MANIFEST=$(oc create secret --dry-run=client generic alertmanager-default --from-file=alertmanager.yaml -o json)
$ oc patch stf default --type=merge -p '{"spec":{"alertmanagerConfigManifest":'"$CONFIG_MANIFEST"'}}'
----
. Verify that the configuration has been applied to the secret:
+
[NOTE]
There will be a short delay as the operators update each object
+
[source,bash,options="nowrap"]
----
$ oc get secret alertmanager-default -o go-template='{{index .data "alertmanager.yaml" | base64decode }}'

global:
  resolve_timeout: 10m
  slack_api_url: <slack_api_url>
receivers:
  - name: slack
    slack_configs:
    - channel: #stf-alerts
      title: |-
        ...
      text: >-
        ...
route:
  group_by: ['job']
  group_wait: 30s
  group_interval: 5m
  repeat_interval: 12h
  receiver: 'slack'
----


. Run the `curl` command against the `alertmanager-proxy` service to retrieve the status and `configYAML` contents, and verify that the supplied configuration matches the configuration in Alertmanager:
+
[source,bash,options="nowrap"]
----
$ oc run curl -it --serviceaccount=prometheus-k8s --restart='Never' --image=radial/busyboxplus:curl -- sh -c "curl -k -H \"Content-Type: application/json\" -H \"Authorization: Bearer \$(cat /var/run/secrets/kubernetes.io/serviceaccount/token)\" https://default-alertmanager-proxy:9095/api/v1/status"

{"status":"success","data":{"configYAML":"...",...}}
----

. Verify that the `configYAML` field contains the changes you expect.


. To clean up the environment, delete the `curl` pod:
+
[source,bash]
----
$ oc delete pod curl

pod "curl" deleted
----

.Additional resources

* For more information about the {OpenShift} secret and the Prometheus operator, see https://github.com/coreos/prometheus-operator/blob/master/Documentation/user-guides/alerting.md[Prometheus user guide on alerting].
